{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqqq pip --progress-bar off\n",
        "!pip install -qqq torch==2.1.2 --progress-bar off\n",
        "!pip install -qqq transformers==4.36.2 --progress-bar off\n",
        "!pip install -qqq einops==0.7.0 --progress-bar off\n",
        "!pip install -qqq accelerate==0.25.0 --progress-bar off"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC3exNqytqiE",
        "outputId": "982dfa12-f54f-46e5-828c-c6d5d0726e60"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from inspect import cleandoc # The inspect.cleandoc function from the Python inspect module is a handy tool for working with multi-line docstrings or code blocks.\n",
        "\n"
      ],
      "metadata": {
        "id": "kgBpmLMVuChm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "MODEL_NAME = \"microsoft/phi-2\"\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=\"auto\", #Automatically determines the optimal data type for tensors based on the hardware and available memory.\n",
        "    # flash_attn=True, # These enable Flash Attention and Flash Rotary functions for potential performance improvements.\n",
        "    # flash_rotary=True, # These enable Flash Attention and Flash Rotary functions for potential performance improvements.\n",
        "    # fused_dense=True, # Merges certain operations for potential speedups.\n",
        "    device_map=\"auto\", # Automatically determines the best device placement for computations.\n",
        "    trust_remote_code=True, #This allows for remote code execution, but should be used with extreme care.\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# \"\"\"\n",
        "# Key points to note\n",
        "\n",
        "# The AutoModelForCausal LM class is designed for loading and using pre-trained causal language models.\n",
        "# The from_pretrained() method simplifies the loading process and provides configuration options.\n",
        "# The options torch_dtype, device_map, and trust_remote_code are related to performance and security.\n",
        "\n",
        "# \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "75aa051f273d438fa924dd3af9225deb",
            "0ddc34c95d5144288055ce315427f49c",
            "499cb27051c14231900a9bc13ce483bf",
            "94d4cc02e9104a1a8998052071383a1f",
            "3fde6ca838de435885b4b42609cc40ad",
            "506152bf21024150a6b9c510c5381f92",
            "ac11a76cf0f742b086edc66f4a05bca3",
            "124ba6ed9c2b4b98b59c048cbf083e64",
            "8a6d30a2edff436e97701d7a85838d57",
            "c9eef0ec7dc244dcacdba01d235a5e43",
            "a990f8c1309741f18b74f695bdf520f5"
          ]
        },
        "id": "ImyjFW1qvSIG",
        "outputId": "19611eb1-c71d-4b15-98e9-ed16c8cf80b8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75aa051f273d438fa924dd3af9225deb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "\n",
        "# \"\"\"\n",
        "# Key Components:\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(...): This part creates a tokenizer object using the AutoTokenizer class and loads it from a pre-trained source.\n",
        "# MODEL_NAME: This placeholder represents the name or path of the pre-trained model or tokenizer you want to load.\n",
        "# trust_remote_code=True: This optional parameter allows for remote code execution when loading the tokenizer.\n",
        "# \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5w7qhWMwXpE",
        "outputId": "d9ff51c5-e2c9-4bea-b231-568ba73ec738"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GenerationConfig # Holds and manages settings for generating text with models trained for tasks like summarization, translation, and open-ended text creation.\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "generation_config.max_new_tokens = 1024 # Limits the length of the generated text to a maximum of 1024 words or subwords.\n",
        "generation_config.temperature = 0.0001 # Produces highly deterministic and less creative text, as the model will strongly favor the most likely words at each step.\n",
        "generation_config.do_sample = True # Introduces some randomness, even with the low temperature, so there might be slight variations in the generated text despite the deterministic tendency.\n",
        "\n",
        "\n",
        "\n",
        "# \"\"\"\n",
        "# Customize configuration:\n",
        "\n",
        "# generation_config.max_new_tokens = 1024\n",
        "# Sets the maximum number of new tokens to generate (excluding the tokens in the input prompt). In this case, it's set to 1024, meaning the generated text will contain a maximum of 1024 new words or subwords.\n",
        "# generation_config.temperature = 0.0001\n",
        "# Controls the randomness of the generated text. Lower values (like 0.0001) make the output more deterministic and repetitive, while higher values introduce more creativity and variation.\n",
        "# generation_config.do_sample = True\n",
        "# Enables random sampling during generation, allowing for more diverse and unpredictable text sequences.\n",
        "\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "aoa2vufEwwPH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer # The TextStreamer object is designed to stream generated text in real-time, as it's produced by the model.\n",
        "\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "# \"\"\"\n",
        "# tokenizer: This is the tokenizer object associated with the model you'll be using for text generation. It's essential for decoding generated tokens into text.\n",
        "# skip_prompt=True: This option tells the streamer to not include the input prompt in the streamed output. It will only stream the newly generated text.\n",
        "# skip_special_tokens=True: This option instructs the streamer to ignore special tokens (like those used for padding or marking sentence boundaries) when streaming text. This ensures a cleaner and more readable output.\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "vvWpiIRD0vtF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline # This function enables you to easily create pipelines for various NLP tasks, including text generation. Think of it as a pre-built tool you can use without diving into the complexities of the underlying model and tokenizer.\n",
        "\n",
        "\n",
        "llm = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True, # This tells the pipeline to return the complete generated text, including any prompt you provide and special tokens used by the model. By default, it might only return the part that's newly generated.\n",
        "    generation_config=generation_config,\n",
        "    num_return_sequences=1, # This specifies how many different generated sequences you want the pipeline to produce. In this case, it's set to 1, so you'll only get one output text.\n",
        "    # eos_token_id=tokenizer.eos_token_id, # This tells the pipeline which token ID signifies the end of a sentence. This helps ensure proper segmentation of the output text.\n",
        "    # pad_token_id=tokenizer.pad_token_id, # This provides the ID of the padding token, which is used to pad shorter inputs to a common length for processing.\n",
        "    streamer=streamer\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "ibX0pWCC-dEI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You're helpful assistant that always answers truthfully.\n",
        "\"\"\".strip()\n",
        "\n",
        "def create_prompt(prompt: str, system_prompt: str = SYSTEM_PROMPT) -> str:\n",
        "\n",
        "  if not system_prompt:\n",
        "    return cleandoc(\n",
        "        f\"\"\"\n",
        "    Instruct: {prompt}\n",
        "    Output:\n",
        "    \"\"\"\n",
        "    )\n",
        "\n",
        "  return cleandoc(\n",
        "      f\"\"\"\n",
        "  Instruct: {system_prompt} {prompt}\n",
        "  Output:\n",
        "  \"\"\"\n",
        "  )\n",
        "\n",
        "prompt = create_prompt(\"What are the pros/cons of ChatGPT vs Open Source LLMs?\")\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDSDg5VAUxZQ",
        "outputId": "33f2472b-b9c5-4fb7-e40a-d488c13a899b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruct: You're helpful assistant that always answers truthfully. What are the pros/cons of ChatGPT vs Open Source LLMs?\n",
            "Output: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# ouput = llm(create_prompt(\"What are the pros/cons of ChatGPT vs Open Source LLMs?\"))"
      ],
      "metadata": {
        "id": "SUmXRoS8ZXwM",
        "outputId": "6dc1b651-ce78-488b-e77f-986f89eff1a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 7.87 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EdbVz7Y1Zow_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "75aa051f273d438fa924dd3af9225deb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ddc34c95d5144288055ce315427f49c",
              "IPY_MODEL_499cb27051c14231900a9bc13ce483bf",
              "IPY_MODEL_94d4cc02e9104a1a8998052071383a1f"
            ],
            "layout": "IPY_MODEL_3fde6ca838de435885b4b42609cc40ad"
          }
        },
        "0ddc34c95d5144288055ce315427f49c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_506152bf21024150a6b9c510c5381f92",
            "placeholder": "​",
            "style": "IPY_MODEL_ac11a76cf0f742b086edc66f4a05bca3",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "499cb27051c14231900a9bc13ce483bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_124ba6ed9c2b4b98b59c048cbf083e64",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a6d30a2edff436e97701d7a85838d57",
            "value": 2
          }
        },
        "94d4cc02e9104a1a8998052071383a1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9eef0ec7dc244dcacdba01d235a5e43",
            "placeholder": "​",
            "style": "IPY_MODEL_a990f8c1309741f18b74f695bdf520f5",
            "value": " 2/2 [00:00&lt;00:00,  2.83it/s]"
          }
        },
        "3fde6ca838de435885b4b42609cc40ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "506152bf21024150a6b9c510c5381f92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac11a76cf0f742b086edc66f4a05bca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "124ba6ed9c2b4b98b59c048cbf083e64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a6d30a2edff436e97701d7a85838d57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c9eef0ec7dc244dcacdba01d235a5e43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a990f8c1309741f18b74f695bdf520f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}